{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../patent_data/cinpatent/en_patent/segmentation/cinpatent_en_0.05.ndjson'\n",
    "output_dir = './datasets/en_patent_0.05'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "lang = 'ja'\n",
    "feat_list = ['title', 'abstract', 'claim_1', 'description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples 11349\n"
     ]
    }
   ],
   "source": [
    "with open(data_path, 'r') as f:\n",
    "    samples = [json.loads(line) for line in f]\n",
    "print('Number of samples', len(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels: 425\n"
     ]
    }
   ],
   "source": [
    "data = {'train': [], 'val': [], 'test': []}\n",
    "all_labels = set()\n",
    "\n",
    "for x in samples:\n",
    "    if x['is_train']:\n",
    "        data['train'].append(x)\n",
    "    elif x['is_dev']:\n",
    "        data['val'].append(x)\n",
    "    else:\n",
    "        data['test'].append(x)\n",
    "    all_labels.update(x['labels'])\n",
    "\n",
    "all_labels = {x: i for i, x in enumerate(sorted(all_labels))}\n",
    "print('Number of labels:', len(all_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(sample, feat_list):\n",
    "    text = []\n",
    "    for feat in feat_list:\n",
    "        text.extend(sample[feat].split())\n",
    "    res = \" \".join(text).lower()[:5000]\n",
    "    return res if len(res) != 0 else \"none\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(sample, all_labels):\n",
    "    encoded_labels = [str(all_labels[x]) for x in sample['labels']]\n",
    "    return ','.join(encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english') if lang == 'en' else None\n",
    "vectorizer = TfidfVectorizer(tokenizer=lambda x: x.split(), stop_words=stop_words, min_df=5, max_df=0.8, max_features=50000)\n",
    "\n",
    "X, y = {}, {}\n",
    "X['train'] = vectorizer.fit_transform([get_text(sample, feat_list=feat_list) for sample in data['train']])\n",
    "y['train'] = [get_labels(sample, all_labels) for sample in data['train']]\n",
    "X['val'] = vectorizer.transform([get_text(sample, feat_list=feat_list) for sample in data['val']])\n",
    "y['val'] = [get_labels(sample, all_labels) for sample in data['val']]\n",
    "X['test'] = vectorizer.transform([get_text(sample, feat_list=feat_list) for sample in data['test']])\n",
    "y['test'] = [get_labels(sample, all_labels) for sample in data['test']]\n",
    "n_features = len(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output train\n",
      "Output val\n",
      "Output test\n"
     ]
    }
   ],
   "source": [
    "for data_set in ('train', 'val', 'test'):\n",
    "    print('Output', data_set)\n",
    "    with open(os.path.join(output_dir, f'{data_set}.txt'), 'w') as f:\n",
    "        n_samples = len(data[data_set])\n",
    "        f.write('{} {} {}\\n'.format(n_samples, n_features, len(all_labels)))\n",
    "        for i, ey in enumerate(y[data_set]):\n",
    "            f.write(ey)\n",
    "            for feat_id, feat_val in zip(\n",
    "                X[data_set].indices[X[data_set].indptr[i]:X[data_set].indptr[i+1]], \n",
    "                X[data_set].data[X[data_set].indptr[i]:X[data_set].indptr[i+1]]\n",
    "            ):\n",
    "                f.write(\" {}:{}\".format(feat_id, feat_val))\n",
    "            f.write('\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "782813c1ee7d94e6a7e592839f3428b07ad94676f042cceb91a0a4cb60354c25"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
